<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-6864-2');
  </script>

  <title>Taimoor Tariq</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href="https://fonts.cdnfonts.com/css/linux-libertine" rel="stylesheet">
  
  <meta name="author" content="Taimoor Tariq">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" type="image/png" href="usi_logo.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EWEXX70BKL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EWEXX70BKL');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Taimoor Tariq</name>
              </p>
              <p>
                Scientist+Engineer interested in understanding and maximizing the perceived quality of visual experiances. I currently work on &#128248 Algorithms at Apple ï£¿. 
                I did my PhD under the mentorship ofÂ <a href="https://www.pdf.inf.usi.ch/people/piotr/">Piotr Didyk</a>, working towards realizing the dream of real-time AR/VR that is visually indistinguishable from the real world. 
                I have also worked with the Applied Perception Science and Display Systems Research teams atÂ MetaÂ on real-time perceptually optimized computational display algorithms for AR/VR. 
                During my Master's, I was a research fellow at KAIST, investigating how image/video enhancement neural networks understand visual quality, and how we can teach them to perceive it the same way humans do.
                <p style="text-align:center">
                <a href="CV_taimoor.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mDWSp-YAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://flickr.com/photos/200857901@N06">Photography ðŸ“· </a> &nbsp/&nbsp
                <a href="https://twitter.com/TaimoorTariq95">Twitter </a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="me.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="me.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <hr>
       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                  My primary interests lie in the intersection of human vision science and real-time computer graphics. I work on understanding,
                  quantifying and maximizing perceptual realism, accurate reproduction, and quality (with constituents such as spatial quality, dynamic range, depth, motion and color) for real-time image/video capture (computational photography), synthesis
                  (rendering/graphics) and display (computational display). The long term goals I aim to push towards are: 
                  <ul>
                    <li>A comprehensive understanding of how human vison understands visual realism and quality.</li>
                    <li>Real-time immersive displays (VR/AR) that are perceptually indistinguishable from the real-world.</li>
                    <li>Real-time smartphone cameras that can not only capture the world exactly as our eyes are seeing see it, but also automatically optimize for aesthetic attiributes/intent in a subjective manner.</li>
                  </ul>
              </p>
            </td>
          </tr>
         
        </tbody></table>
        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Recent News</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><strong><font color="red">[Apr-2025]</font> Our work on depth and distance perception in AR/VR to be presented at SIGGRAPH 2025.</li>
              <li><strong><font color="red">[Oct-2024]</font> I have joined the Camera Algorithms Team at Apple full-time.</li>
              <li><strong><font color="red">[Mar-2024]</font> Our work on preserving motion perception in AR/VR to be presented at <b>SIGGRAPH 2024</b>.</li>
              <li><strong><font color="red">[Feb-2024]</font> Gave an invited talk at University College London (UCL) on my work on Perceptual Optimization of Realism for real-time AR/VR. Thank you <a href="https://kaanaksit.com">Kaan Aksit</a> for the invitation. </li>
              <li><strong><font color="red">[Aug-2023]</font> Our work on ultra-fast perceptually adaptive tone mapping on VR headsets to be presented at <b>SIGGRAPH Asia 2023</b>.</li>
              <li><strong><font color="red">[Oct-2022]</font> I have joined the Applied Perception Science team at <b>Facebook Reality Labs</b> (Sunnyvale, CA) as a Research Scientist Intern.</li>
              <li><strong><font color="red">[Apr-2022]</font> Our work on perceptual enhancement for real-time AR/VR to be presented at <b>SIGGRAPH 2022</b>.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p style="font-weight:normal">
                  Representative projects are highlighted, a few that I consider my most original and fundemental ideas. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='hvssr.JPEG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2411.17513">
                    <papertitle>Human Vision Constrained Super Resolution</papertitle>
                </a>
                <br>

                <a href="https://ch.linkedin.com/in/karpev98">Volodymr Karpenko</a>,
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?user=xaYzz0AAAAAJ&hl=en">Jorge Condor</a>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <p style="font-weight:normal">
                  A step towards practical and efficient real-time deep learning based image/video enhancement by leveraging the limitations of human vision. We design a perceptual model and pipeline to control the local quality and deep learning resources for image/video super resolution, such that there is no visible loss in quality, even with a very significant reduction in computational cost and runtime. The framework is general and can be applied to any deep learning based super resolution achitecture. The framework can also be applied for efficient deep denoising, frame-interpolation, and possibly generative image/video synthesis without noticeable quality loss. 
                </p>
                   <p style="color:#808080">ICCV 2025 - Human Vision Inspired Computer Vision Workshop</p>
                </p>
                </p>
            </td>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='fovdepth.PNG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2501.18635">
                    <papertitle>Towards Understanding Depth Perception in Foveated Rendering</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=f-jf8tIAAAAJ&hl=de">Sophie KergaÃŸner</a>,
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <br>
                <p style="font-weight:normal">
                   Understanding stereoscopic depth perception in Foveated AR/VR. We derive a perceptual model which demonstrates that the blur intensities applied in common foveation procedures do not affect stereoacuity. Additionally, our findings suggest that it is important to maintain high depth quality even in strongly foveated stereoscopic content.
                </p>
                   <p style="color:#808080">SIGGRAPH 2025</p>
                </p>
                </p>
            </td>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='mm_teaser.PNG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="MM.pdf">
                    <papertitle>Towards Motion Metamers for Foveated Rendering</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <br>
                <p style="font-weight:normal">
                  We demonstrate that foveated rendering may inhibit motion perception, making AR/VR appear slower than it physically is. We propose the theory of Motion Metamers of human vision; videos that are structurally different from one another but indistinguishable to human peripheral vision in both spatial and motion perception. We present the first technique to synthesize motion metamers for AR/VR; all in real-time and completely unsupervised (no high-quality reference required). 
                </p>
                   <p style="color:#808080">SIGGRAPH 2024</p>
                </p>
                <div class="paper" id="xie2020advprop">
                  <a href="https://www.youtube.com/watch?v=HVWD2V_hA-A&t=293s">Talk Recording</a> 
                </div>
                </p>
            </td>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='tmo_pic.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="VRTMO.pdf">
                    <papertitle>Perceptually Adaptive Real-Time Tone Mapping</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://www.nathanmatsuda.com/">Nathan Matsuda</a>,
                <a href="https://www.linkedin.com/in/ericpenner">Eric Penner</a>,
                <a href="https://www.linkedin.com/in/jerry-jia-a7553312">Jerry Jia</a>,
                <a href="https://scholar.google.com/citations?user=-qncsGYAAAAJ&hl=en">Douglas Lanman</a>,
                <a href="https://ieeexplore.ieee.org/author/37088511504">Ajit Ninan</a>,
                <a href="https://achapiro.github.io/">Alexandre Chapiro</a>
                <br>
                <p style="font-weight:normal">
                  An ultra-fast (under 1ms per-frame on standalone VR) framework that adaptively maintains the perceptual appearence of HDR content after tone-mapping. The framework relates human contrast perception across very different lumainances scales, and then optimizes any tone-mapping curve to minimize perceptual difference. 
                </p>
                   <p style="color:#808080">SIGGRAPH Asia 2023</p>
                </p>
                </p>
            </td>
    
          <tr>
          <tr bgcolor="#ffffd0">
            
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='noiseGIFfinal.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.pdf.inf.usi.ch/projects/NoiseBasedEnhancement/NoiseBasedEnhancement.pdf">
                    <papertitle>Noise-based Enhancement for Foveated Rendering</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=UvhjCNEAAAAJ">Cara Tursun</a>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <br>  
                <p style="font-weight:normal">
                   The fastest (200FPS at 4K) and first no-reference spatial metamers of human peripheral vision that we know of; specifically tailored for direct integration into the real-time VR foveated rendering pipeline. Save upto 40% (rendering time) over tranditional foveated rendering, without visible loss in quality.  
                </p>
                   <p style="color:#808080">SIGGRAPH 2022</p>
                </p>
                <div class="paper" id="xie2020advprop">
                  <a href="https://www.pdf.inf.usi.ch/projects/NoiseBasedEnhancement/index.html">Project Page</a> 
                </div>
                </p>
            </td>
    
            <tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='DeepFeatures.JPG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/1812.00412.pdf">
                    <papertitle>Why Are Deep Representations Good Perceptual Quality Features?</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=UvhjCNEAAAAJ">Okan Tarhan Tursun</a>,
                <a href="https://scholar.google.com/citations?user=bGXte_4AAAAJ&hl=en">Munchurl Kim</a>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <p style="font-weight:normal">
                  An investigation into why the representations learned by image recognition CNNs work remarkably well as features of perceptual quality (e.g perceptual loss). We theorize that these image classification representations learn to be spectrally sensitive to the same spatial frequencies which the human visual system is most sensitive to, so they can effectively encode perceptually visible distortions.  
                </p>
                   <p style="color:#808080">ECCV 2020</p>
                </p>
                <div class="paper" id="xie2020advprop">
                    <a href="https://www.youtube.com/watch?v=v92gsK4YY8M">Fast-Forward Video</a> 
                </div>
                </p>
            </td>
      
        </tr> <!--xie2020sat-->
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='hvs_att.PNG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Tariq_A_HVS-Inspired_Attention_to_Improve_Loss_Metrics_for_CNN-Based_Perception-Oriented_ICCVW_2019_paper.pdf">
                    <papertitle>A HVS-Inspired Attention to Improve Loss Metrics for CNN-Based Perception-Oriented Super-Resolution</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?user=7wAFtjIAAAAJ&hl=en">Juan Luis Gonzalez Bello</a>,
                <a href="https://scholar.google.com/citations?user=bGXte_4AAAAJ&hl=en">Munchurl Kim</a>
                <p style="font-weight:normal">
                  A human contrast perception inspired spatial attention mask that makes the deep learning pipeline aware of perceptually important visual information in images.  
                </p>
                <p style="color:#808080">ICCV 2019 - Learning for Computational Imaging Workshop</p>
                </p>
            </td>
        </tr> <!--xie2020sat-->
  
        </tr> <!--xie2020sat-->
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='brain_ai.jpeg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.sciencedirect.com/science/article/pii/S0169260719304754">
                    <papertitle>Computationally efficient fully-automatic online neural spike detection and sorting in presence of multi-unit activity for implantable circuits</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=hfoiT68AAAAJ">Muhammad Hashim Satti</a>,
                <a href="https://sites.google.com/site/hamidmkamboh/">Hamid Mehmood Kamboh</a>,
                <a href="https://scholar.google.com/citations?user=qzVv34MAAAAJ&hl=en">Maryam Saeed</a>,
                <a href="https://scholar.google.com/citations?user=G-cEFxIAAAAJ&hl=en">Awais Mehmood Kamboh</a>
                <p style="font-weight:normal">
                  A signal processing pipeline for unsupervised sorting of brain signals on impalntable neural chips, primarily for neuro-prosthetics.   
                </p>
                <p style="color:#808080">Computer Methods and Programs in Biomedicine, 2019</p>
                </p>
            </td>
        </tr> <!--xie2020sat-->

        </tr> <!--xie2020sat-->
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='neuron_ai.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/abstract/document/8037013">
                    <papertitle>Low SNR neural spike detection using scaled energy operators for implantable brain circuits</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=hfoiT68AAAAJ">Muhammad Hashim Satti</a>,
                <a href="https://scholar.google.com/citations?user=qzVv34MAAAAJ&hl=en">Maryam Saeed</a>,
                <a href="https://scholar.google.com/citations?user=G-cEFxIAAAAJ&hl=en">Awais Mehmood Kamboh</a>
                <p style="font-weight:normal">
                   A new non-linear signal processing filter for detecting noisy brain action potentials.    
                </p>
                <p style="color:#808080">IEEE Engineering in Medicine and Biology Conference (EMBC), 2017</p>
                </p>
            </td>
        </tr> <!--xie2020sat-->
        
        </tbody></table>
        <hr>


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Service</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="old-images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
        

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-6864-3', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
