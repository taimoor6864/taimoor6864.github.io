<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-6864-2');
  </script>

  <title>Taimoor Tariq</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href="https://fonts.cdnfonts.com/css/linux-libertine" rel="stylesheet">
  
  <meta name="author" content="Taimoor Tariq">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" type="image/png" href="usi_logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Taimoor Tariq</name>
              </p>
              <p>I am a PhD student at the <a href="https://www.pdf.inf.usi.ch/index.html">USI Lugano, Switzerland</a>, working with <a href="https://scholar.google.com/citations?user=kH5VxAIAAAA">Piotr Didyk</a>.
              I am working on the ERC Starting Grant supported <a href="https://cordis.europa.eu/project/id/804226">PERDY</a> project, focused on display-specific perceptual optimization of graphics content to match the requirements of human perception.
              I received my M.S. degree from KAIST, working with the <a href="https://www.viclab.kaist.ac.kr/">Image and Video Computing Group</a>. 
              During my PhD, I have also worked with the Applied Perception Science team at <a href="https://about.meta.com/realitylabs/">Facebook Reality Labs</a>, on real-time computational display algorithms for Virtual Reality displays.<p>
                <p style="text-align:center">
                <a href="mailto:taimoor.tariq@usi.ch">Email</a> &nbsp/&nbsp
                <a href="CV_TT.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mDWSp-YAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://500px.com/p/13beettariq">500px ðŸ“· </a> &nbsp/&nbsp
              </p>
              <p style="text-align:center">
                <a href="https://twitter.com/TaimoorTariq95" class="twitter-follow-button" data-show-count="false">Follow @TaimoorTariq95</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="unnamed_tt.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="unnamed_tt.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <hr>
       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                  I enjoy demystifying and modelling how our eyes percieve the visual world, and my research interests lie in the intersection of vision science and computer graphics. More specifically, I work on understanding,
                  quantifying and maximizing PERCEIVED visual realism for capture (camera processing pipeline), synthesis
                  (rendering/graphics pipeline) and display (computational display). The long term goals I aim to push towards are; to advance our fundemental understanding of human perception and cognition, and apply this understanding to enable real-time immersive display techniques (VR/AR) that are indistunguisbile from the real-world. 
              </p>
            </td>
          </tr>
         
        </tbody></table>
        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Recent News</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><strong><font color="red">[Mar-2024]</font> Our work on preserving motion perception in AR/VR to be presented at <b>SIGGRAPH 2024</b>.</li>
              <li><strong><font color="red">[Feb-2024]</font> Gave a talk at UCL on my work on Perceptual Optimization of Realism for real-time AR/VR.</li>
              <li><strong><font color="red">[Aug-2023]</font> Our work on ultra-fast perceptually adaptive tone mapping on VR-HMDs to be presented at <b>SIGGRAPH Asia 2023</b>.</li>
              <li><strong><font color="red">[Oct-2022]</font> I have joined the Applied Perception Science team at <b>Facebook Reality Labs</b> (Sunnyvale, CA) as a Research Scientist Intern.</li>
              <li><strong><font color="red">[Apr-2022]</font> Our work on perceptual enhancement for real-time AR/VR to be presented at <b>SIGGRAPH 2022</b>.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p style="font-weight:normal">
                  Representative projects are highlighted. 
                </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='tmo_pic.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="VRTMO.pdf">
                    <papertitle>Perceptually Adaptive Real-Time Tone Mapping</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://www.nathanmatsuda.com/">Nathan Matsuda</a>,
                <a href="https://www.linkedin.com/in/ericpenner">Eric Penner</a>,
                <a href="https://www.linkedin.com/in/jerry-jia-a7553312">Jerry Jia</a>,
                <a href="https://scholar.google.com/citations?user=-qncsGYAAAAJ&hl=en">Douglas Lanman</a>,
                <a href="https://ieeexplore.ieee.org/author/37088511504">Ajit Ninan</a>,
                <a href="https://achapiro.github.io/">Alexandre Chapiro</a>
                <br>
                <p style="font-weight:normal">
                  An ultra-fast (under 1ms per-frame) framework that adaptively maintains the perceptual appearence of HDR content after tone-mapping. The framework relates human contrast perception across very different lumainances scales, and then optimizes any tone-mapping curve to minimize perceptual difference. 
                </p>
                   <p style="color:#808080">SIGGRAPH Asia 2023</p>
                </p>
                </p>
            </td>
    
          <tr>
          <tr bgcolor="#ffffd0">
            
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='noiseGIFfinal.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.pdf.inf.usi.ch/projects/NoiseBasedEnhancement/NoiseBasedEnhancement.pdf">
                    <papertitle>Noise-based Enhancement for Foveated Rendering</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=UvhjCNEAAAAJ">Cara Tursun</a>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <br>  
                <p style="font-weight:normal">
                   The fastest (200FPS at 4K) and first no-reference spatial metamers of human peripheral vision that we know of; specifically tailored for direct integration into the real-time VR foveated rendering pipeline. Save upto 40% (rendering time) over tranditional foveated rendering, without visible loss in quality.  
                </p>
                   <p style="color:#808080">SIGGRAPH 2022 [journal]</p>
                </p>
                <div class="paper" id="xie2020advprop">
                  <a href="https://www.pdf.inf.usi.ch/projects/NoiseBasedEnhancement/index.html">Project Page</a> 
                </div>
                </p>
            </td>
    
            <tr>
            <tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='DeepFeatures.JPG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/1812.00412.pdf">
                    <papertitle>Why Are Deep Representations Good Perceptual Quality Features?</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=UvhjCNEAAAAJ">Okan Tarhan Tursun</a>,
                <a href="https://scholar.google.com/citations?user=bGXte_4AAAAJ&hl=en">Munchurl Kim</a>,
                <a href="https://scholar.google.com/citations?user=kH5VxAIAAAAJ&hl=en">Piotr Didyk</a>
                <p style="font-weight:normal">
                  An investigation into why the representations learned by image recognition CNNs work remarkably well as features of perceptual quality (e.g perceptual loss). We theorize that these image classification representations learn to be spectrally sensitive to the same spatial frequencies which the human visual system is most sensitive to, so they can effectively encode perceptually visible distortions.  
                </p>
                   <p style="color:#808080">European Conference on Computer Vision (ECCV), 2020</p>
                </p>
                <div class="paper" id="xie2020advprop">
                    <a href="https://www.youtube.com/watch?v=v92gsK4YY8M">Fast-Forward Video</a> 
                </div>
                </p>
            </td>
      
        </tr> <!--xie2020sat-->
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='hvs_att.PNG' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Tariq_A_HVS-Inspired_Attention_to_Improve_Loss_Metrics_for_CNN-Based_Perception-Oriented_ICCVW_2019_paper.pdf">
                    <papertitle>A HVS-Inspired Attention to Improve Loss Metrics for CNN-Based Perception-Oriented Super-Resolution</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?user=7wAFtjIAAAAJ&hl=en">Juan Luis Gonzalez Bello</a>,
                <a href="https://scholar.google.com/citations?user=bGXte_4AAAAJ&hl=en">Munchurl Kim</a>
                <p style="font-weight:normal">
                  A human contrast perception inspired spatial attention mask that makes the deep learning pipeline aware of perceptually important visual information in images.  
                </p>
                <p style="color:#808080">International Conference on Computer Vision Workshops (ICCVW), 2019</p>
                </p>
            </td>
        </tr> <!--xie2020sat-->
  
        </tr> <!--xie2020sat-->
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='brain_ai.jpeg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.sciencedirect.com/science/article/pii/S0169260719304754">
                    <papertitle>Computationally efficient fully-automatic online neural spike detection and sorting in presence of multi-unit activity for implantable circuits</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=hfoiT68AAAAJ">Muhammad Hashim Satti</a>,
                <a href="https://sites.google.com/site/hamidmkamboh/">Hamid Mehmood Kamboh</a>,
                <a href="https://scholar.google.com/citations?user=qzVv34MAAAAJ&hl=en">Maryam Saeed</a>,
                <a href="https://scholar.google.com/citations?user=G-cEFxIAAAAJ&hl=en">Awais Mehmood Kamboh</a>
                <p style="font-weight:normal">
                  A signal processing pipeline for unsupervised sorting of brain signals on impalntable neural chips, primarily for neuro-prosthetics.   
                </p>
                <p style="color:#808080">Computer Methods and Programs in Biomedicine, 2019</p>
                </p>
            </td>
        </tr> <!--xie2020sat-->

        </tr> <!--xie2020sat-->
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='neuron_ai.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/abstract/document/8037013">
                    <papertitle>Low SNR neural spike detection using scaled energy operators for implantable brain circuits</papertitle>
                </a>
                <br>
                <strong>Taimoor Tariq</strong>,
                <a href="https://scholar.google.com/citations?hl=en&user=hfoiT68AAAAJ">Muhammad Hashim Satti</a>,
                <a href="https://scholar.google.com/citations?user=qzVv34MAAAAJ&hl=en">Maryam Saeed</a>,
                <a href="https://scholar.google.com/citations?user=G-cEFxIAAAAJ&hl=en">Awais Mehmood Kamboh</a>
                <p style="font-weight:normal">
                   A new non-linear signal processing filter for detecting noisy brain action potentials.    
                </p>
                <p style="color:#808080">IEEE Engineering in Medicine and Biology Conference (EMBC), 2017</p>
                </p>
            </td>
        </tr> <!--xie2020sat-->
        
        </tbody></table>
        <hr>


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Service</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="old-images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
        

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-6864-3', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
